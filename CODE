#SARCASM CODE

import pandas as pd
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
import joblib

# Step 1: Load dataset
df = pd.read_csv("/content/sarcasm_dataset_500.csv")
print("Dataset Loaded Successfully!")

# Step 2: Split data
X = df['text']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: TF-IDF Vectorization
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 2))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 4: Train Logistic Regression Model
model = LogisticRegression(max_iter=1000, C=2)
model.fit(X_train_tfidf, y_train)

# Step 5: Evaluate
y_pred = model.predict(X_test_tfidf)
print("Accuracy:", round(accuracy_score(y_test, y_pred), 3))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Step 6: Save Model
joblib.dump(model, "sarcasm_model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")

# Step 7: Predict Function
def predict_sarcasm(comment):
    sarcasm_keywords = [
        "yeah right", "oh great", "of course", "just what i needed",
        "sure thing", "as if", "can't wait", "amazing not", 
        "totally helpful", "wonderful news", "nice job", "brilliant idea"
    ]
    comment_clean = comment.lower().strip()
    if any(phrase in comment_clean for phrase in sarcasm_keywords):
        return "Sarcastic"
    comment_vec = vectorizer.transform([comment])
    prediction = model.predict(comment_vec)[0]
    return "Sarcastic" if prediction == 1 else "Not Sarcastic"


#EMOTION CODE 
import re
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import LinearSVC
from sklearn.feature_extraction import DictVectorizer
import joblib

# Step 1: Read dataset
def read_data(file):
    data = []
    with open(file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            label = ' '.join(line[1:line.find("]")].strip().split())
            text = line[line.find("]")+1:].strip()
            data.append([label, text])
    return data

# Step 2: Feature extraction using n-grams
def ngram(token, n):
    return [' '.join(token[i-n+1:i+1]) for i in range(n-1, len(token))]

def create_feature(text, nrange=(1, 3)):
    text = text.lower()
    text = re.sub(r"i['’]m", "i am", text)
    text = re.sub(r"can['’]t", "cannot", text)
    text = re.sub(r"n['’]t", " not", text)
    text_alphanum = re.sub('[^a-z0-9#]', ' ', text)
    text_punc = re.sub('[a-z0-9]', ' ', text)
    features = []
    for n in range(nrange[0], nrange[1] + 1):
        features += ngram(text_alphanum.split(), n)
    features += ngram(text_punc.split(), 1)
    return Counter(features)

# Step 3: Train Model
file = "/content/emotions_dataset_realistic_ordered.txt"
data = read_data(file)
emotions = ["joy", "fear", "anger", "sadness", "disgust", "shame", "guilt"]

X_all, y_all = [], []
for label, text in data:
    y_all.append(label)
    X_all.append(create_feature(text, nrange=(1, 3)))

X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.3, random_state=123)
vectorizer = DictVectorizer(sparse=True)
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

clf = LinearSVC(random_state=123, class_weight='balanced')
clf.fit(X_train, y_train)
print(f"Training Accuracy: {accuracy_score(y_train, clf.predict(X_train)):.2f}")
print(f"Test Accuracy: {accuracy_score(y_test, clf.predict(X_test)):.2f}")

# Save Model
joblib.dump(clf, "emotion_model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")

# Prediction
def predict_emotion(sentence):
    features = create_feature(sentence, nrange=(1, 3))
    vec = vectorizer.transform([features])
    return clf.predict(vec)[0]


